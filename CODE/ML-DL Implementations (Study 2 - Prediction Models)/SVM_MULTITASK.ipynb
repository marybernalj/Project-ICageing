{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4wmcN+TYxpedXT3XCxU25"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbe9H0VqrAZ-","executionInfo":{"status":"ok","timestamp":1743964045585,"user_tz":300,"elapsed":22119,"user":{"displayName":"Mary Carlota Bernal","userId":"15602430268156191304"}},"outputId":"d151706b-527a-4a9b-a723-f825bc393cd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["SVM"],"metadata":{"id":"IsIY4pqNhiWL"}},{"cell_type":"code","source":["!pip install scikit-learn\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow import keras\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.optimizers import Adam # Add this line to import the Adam\n","# Split data into training and test sets\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","from sklearn.metrics import mean_absolute_error # Import the mean_absolute_error function\n","from sklearn.svm import SVR\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.pipeline import make_pipeline\n","\n","\n","%load_ext tensorboard\n","\n","import os\n","log_dir = os.path.join(\"logs\") # Puedes cambiar \"logs\" por el nombre que quieras\n","if not os.path.exists(log_dir):\n","  os.makedirs(log_dir)\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","\n","# Cargar los datos (suponiendo que están en un archivo CSV)\n","data = pd.read_csv('/content/drive/My Drive/PREDICTIVE MODEL IC/DATA/DATOS_OBSERVACION_VAR_NOTNULL2010-2022_TRANSFORM.csv')\n","\n","#ARREGLO PARA CALC_IMC NULL\n","data['CALC_IMC'] = data.groupby('ID')['CALC_IMC'].transform(lambda x: round(x.fillna(x.mean()), 0))\n","#eliminar filas con datos nulos en IMC\n","data = data.dropna(subset=['CALC_IMC'])\n","\n","# Contar registros por ID, ignorando los NaN\n","#conteo_registros_por_persona = data.groupby('ID')['ID'].count()\n","\n","# Personas con exactamente 4 registros (sin considerar NaN)\n","#personas_con_4_registros = conteo_registros_por_persona[conteo_registros_por_persona == 4]\n","#print(personas_con_4_registros)\n","\n","data.to_csv('/content/drive/My Drive/PREDICTIVE MODEL IC/DATA/DATOS_OBSERVACION_VAR_NOTNULL2010-2022_TRANSFORM_1.csv', index=False)\n","data = pd.read_csv('/content/drive/My Drive/PREDICTIVE MODEL IC/DATA/DATOS_OBSERVACION_VAR_NOTNULL2010-2022_TRANSFORM_1.csv')\n","\n","# Normalize numerical variables\n","columns_to_scale = ['LOCOMOTION','SENSORY','VITALITY','PSICHOLOGICAL','COGNITION','CIGARRETTES','DRINK','CALC_IMC', 'AGE']\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","#scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(data[columns_to_scale])\n","#data[columns_to_scale] = scaled_data\n","data.loc[:, columns_to_scale] = scaled_data\n","\n","# Convert categorical variables to one-hot encoding\n","# Crear un objeto OneHotEncoder\n","columns_to_transform = ['SEX','ALONE','PHYSICAL_ACTIVITY']\n","encoder = OneHotEncoder(sparse_output=False)\n","encoded_data = encoder.fit_transform(data[columns_to_transform])\n","encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns_to_transform))\n","data = pd.concat([data, encoded_df], axis=1)\n","\n","#data = data.drop(columns=['SEX','ALONE','PHYSICAL_ACTIVITY'])\n","\n","# Agrupar por ID y ordenar por 'Evaluación'\n","#data_grouped = data.sort_values('evaluacion').groupby('ID')\n","\n","#data = data.drop(columns=['ID','año','evaluación'])\n","\n","#data.head()\n","#print(data.columns)\n","\n","data.to_csv('/content/drive/My Drive/PREDICTIVE MODEL IC/DATA/DATOS_OBSERVACION_VAR_NOTNULL2010-2022_TRANSFORM_ENCODED.csv', index=False)\n","\n","# Definimos las columnas de las variables a predecir (target)\n","target_cols = ['LOCOMOTION','SENSORY','VITALITY','PSICHOLOGICAL','COGNITION']\n","\n","# Prepare sequences and targets\n","def create_dataset(data, look_back=3):\n","  sequences = []\n","  targets = []\n","  locomotions = []\n","  sensories = []\n","  vitalities = []\n","  psichologicals = []\n","  cognitions = []\n","  auxiliares = []\n","  for person_id in data['ID'].unique():\n","    person_data = data[data['ID'] == person_id].sort_values('evaluacion')\n","\n","     # Get the indices of the relevant columns\n","    relevant_columns = ['LOCOMOTION', 'SENSORY', 'VITALITY','PSICHOLOGICAL', 'COGNITION',\n","                        'ALONE_PARTNERED', 'ALONE_UNCOUPLED',\n","                        'PHYSICAL_ACTIVITY_SEDENTARY', 'PHYSICAL_ACTIVITY_MILD','PHYSICAL_ACTIVITY_MODERATE', 'PHYSICAL_ACTIVITY_VIGOROUS',\n","                        'CIGARRETTES','DRINK', 'CALC_IMC']\n","    auxiliar_columns = ['AGE','SEX_FEMALE', 'SEX_MALE']\n","     # Select the relevant columns first\n","    person_data_values = person_data[relevant_columns].values\n","    person_data_auxiliar = person_data[auxiliar_columns].values\n","\n","    # Create sequences and targets\n","    if len(person_data_values) < look_back + 1:\n","        continue\n","    sequence = person_data_values[:look_back, :]\n","    target = person_data_values[look_back, data.columns.get_indexer(target_cols)]\n","    locomotion = person_data_values[look_back, 0]\n","    sensory = person_data_values[look_back, 1]\n","    vitality = person_data_values[look_back, 2]\n","    psichological = person_data_values[look_back, 3]\n","    cognition = person_data_values[look_back, 4]\n","    auxiliar = person_data_auxiliar[look_back, :]\n","    sequences.append(sequence)\n","    targets.append(target)\n","    locomotions.append(locomotion)\n","    sensories.append(sensory)\n","    vitalities.append(vitality)\n","    psichologicals.append(psichological)\n","    cognitions.append(cognition)\n","    auxiliares.append(auxiliar)\n","  return np.array(sequences), np.array(targets), np.array(locomotions), np.array(sensories), np.array(vitalities), np.array(psichologicals), np.array(cognitions), np.array(auxiliares)\n","\n","# Create sequences and targets\n","look_back = 3\n","sequences, targets, locomotions, sensories, vitalities, psichologicals, cognitions, auxiliares = create_dataset(data, look_back)\n","\n","pd.DataFrame(sequences[0]).head()\n","sequences.shape\n","sequences.shape[2]\n","\n","sequences.shape[2]\n","#pd.DataFrame(targets[0]).head()\n","\n","auxiliares.shape\n","auxiliares.shape[1]\n","\n","targets.shape\n","\n","# TRAINNING AND TEST\n","\n","# Assuming 'sequences' and 'auxiliares' are your input features,\n","# and 'locomotions', 'sensories', etc. are your targets\n","# Split into 80% training and 20% testing\n","(\n","    sequences_train,\n","    sequences_test,\n","    auxiliares_train,\n","    auxiliares_test,\n","    locomotions_train,\n","    locomotions_test,\n","    sensories_train,\n","    sensories_test,\n","    vitalities_train,\n","    vitalities_test,\n","    psichologicals_train,\n","    psichologicals_test,\n","    cognitions_train,\n","    cognitions_test,\n",") = train_test_split(\n","    sequences,\n","    auxiliares,\n","    locomotions,\n","    sensories,\n","    vitalities,\n","    psichologicals,\n","    cognitions,\n","    test_size=0.2,\n","    random_state=42,  # Optional: for reproducibility\n",")\n","\n","\n","# Define input shapes\n","sequential_input_shape = (sequences.shape[1], sequences.shape[2])  # Adjust num_sequential_features\n","auxiliary_input_shape = (auxiliares.shape[1],)  # Adjust num_auxiliary_features\n","\n","# Create the model\n","def create_model_svm():\n","    # Flatten the sequential data for SVM\n","    sequences_train_flattened = sequences_train.reshape(sequences_train.shape[0], -1)\n","    sequences_test_flattened = sequences_test.reshape(sequences_test.shape[0], -1)\n","\n","    # Create a pipeline with StandardScaler and MultiOutputRegressor for SVM\n","    model = make_pipeline(StandardScaler(),\n","                         MultiOutputRegressor(SVR(kernel='rbf')))\n","    # Combine sequential and auxiliary data for training\n","    X_train = np.concatenate([sequences_train_flattened, auxiliares_train], axis=1)\n","\n","    # Combine target variables for training\n","    y_train = np.column_stack([locomotions_train, sensories_train, vitalities_train,\n","                               psichologicals_train, cognitions_train])\n","\n","    # Fit the SVM model\n","    model.fit(X_train, y_train)\n","\n","    # Combine sequential and auxiliary data for testing (for consistency)\n","    X_test_final = np.concatenate([sequences_test_flattened, auxiliares_test], axis=1)\n","\n","    # Combine target variables for testing (for consistency)\n","    y_test_final = np.column_stack([locomotions_test, sensories_test, vitalities_test,\n","                                  psichologicals_test, cognitions_test])\n","\n","    return model, X_test_final, y_test_final\n","\n","\n","# Función para calcular MAE con intervalo de confianza usando Bootstrap (CÓDIGO NUEVO)\n","def calcular_intervalos_confianza(model, X_test, y_test, num_bootstrap_samples=1000):\n","    bootstrap_maes = []\n","    for _ in range(num_bootstrap_samples):\n","        X_test_resampled, y_test_resampled = resample(X_test, y_test, replace=True)\n","        predictions = model.predict(X_test_resampled)\n","\n","     # Calculate MAE for each target and store them\n","        maes_for_this_bootstrap = [mean_absolute_error(y_test_resampled[:, i], predictions[:, i])\n","                                 for i in range(y_test.shape[1])]\n","        bootstrap_maes.append(maes_for_this_bootstrap)\n","\n","    bootstrap_maes = np.array(bootstrap_maes)\n","    lower_percentile = 2.5  # For 95% CI\n","    upper_percentile = 97.5  # For 95% CI\n","\n","    # Calculate CI directly for MAE values\n","    confidence_intervals = []\n","    for i in range(y_test.shape[1]):\n","        lower_bound = np.percentile(bootstrap_maes[:, i], lower_percentile)\n","        upper_bound = np.percentile(bootstrap_maes[:, i], upper_percentile)\n","        confidence_intervals.append((lower_bound, upper_bound))\n","\n","    return confidence_intervals\n","\n","\n","# Create and train the model\n","model, X_test, y_test = create_model_svm()\n","\n","# Define target names here\n","target_names = ['LOCOMOTION', 'SENSORY', 'VITALITY', 'PSICHOLOGICAL', 'COGNITION']\n","\n","# --- Evaluation before Confidence Intervals ---\n","predictions = model.predict(X_test)  # Make predictions on the original test data\n","\n","# --- Calculate Total Loss ---\n","total_loss = 0\n","for i in range(len(target_names)):\n","    loss_for_target = mean_absolute_error(y_test[:, i], predictions[:, i])\n","    total_loss += loss_for_target\n","\n","total_loss /= len(target_names)  # Average the losses\n","\n","print(f\"Total Loss: {total_loss:.4f}\")\n","\n","\n","# Print evaluation metrics (e.g., MAE)\n","target_names = ['LOCOMOTION', 'SENSORY', 'VITALITY', 'PSICHOLOGICAL', 'COGNITION']\n","for i in range(len(target_names)):\n","    mae = mean_absolute_error(y_test[:, i], predictions[:, i])\n","    print(f\"MAE for {target_names[i]}: {mae:.4f}\")\n","\n","\n","\n","# Calculate confidence intervals\n","confidence_intervals = calcular_intervalos_confianza(model, X_test, y_test)\n","\n","# Make predictions using the trained SVM model\n","predictions = model.predict(X_test)\n","\n","# Evaluate performance (including confidence intervals)\n","for i in range(len(target_names)):\n","    mae = mean_absolute_error(y_test[:, i], predictions[:, i])\n","    lower_bound, upper_bound = confidence_intervals[i]\n","    print(f\"MAE for {target_names[i]}: {mae:.4f} (95% CI: [{lower_bound:.4f}, {upper_bound:.4f}])\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739744178802,"user_tz":300,"elapsed":270975,"user":{"displayName":"Mary Carlota Bernal","userId":"15602430268156191304"}},"outputId":"2bbe9305-dc8d-4960-f505-2c94aa09d713","id":"SX0YiW9Yhkdo","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-6-cabf4bd70dd4>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.         0.81818182 0.90909091 ... 0.72727273 0.54545455 0.81818182]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n","  data.loc[:, columns_to_scale] = scaled_data\n","<ipython-input-6-cabf4bd70dd4>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.61428571 0.65714286 0.71428571 ... 0.32857143 0.38571429 0.44285714]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n","  data.loc[:, columns_to_scale] = scaled_data\n"]},{"output_type":"stream","name":"stdout","text":["Total Loss: 0.1183\n","MAE for LOCOMOTION: 0.1599\n","MAE for SENSORY: 0.1051\n","MAE for VITALITY: 0.1239\n","MAE for PSICHOLOGICAL: 0.1169\n","MAE for COGNITION: 0.0859\n","MAE for LOCOMOTION: 0.1599 (95% CI: [0.1483, 0.1720])\n","MAE for SENSORY: 0.1051 (95% CI: [0.0978, 0.1127])\n","MAE for VITALITY: 0.1239 (95% CI: [0.1163, 0.1313])\n","MAE for PSICHOLOGICAL: 0.1169 (95% CI: [0.1071, 0.1282])\n","MAE for COGNITION: 0.0859 (95% CI: [0.0796, 0.0923])\n"]}]}]}